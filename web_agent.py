#!/usr/bin/env python3
"""
Web Agent æ”¹è¿›ç‰ˆ - åŸºäºæµ‹è¯•ç»“æœä¼˜åŒ–
é›†æˆäº†v1ç‰ˆæœ¬çš„æˆåŠŸé…ç½®
"""

import sys
import subprocess
import argparse
import json
import time
from urllib.parse import urlparse

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    try:
        import selenium, bs4
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", "selenium", "beautifulsoup4", "webdriver-manager"])

check_dependencies()

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
from webdriver_manager.chrome import ChromeDriverManager

def is_social_media(url):
    """æ£€æµ‹ç¤¾äº¤åª’ä½“ç½‘ç«™"""
    domains = ['twitter.com', 'x.com', 'facebook.com', 'instagram.com']
    return any(d in urlparse(url).netloc.lower() for d in domains)

def is_complex_site(url):
    """æ£€æµ‹å¤æ‚ç½‘ç«™(éœ€è¦å®Œæ•´åæ£€æµ‹çš„ç½‘ç«™)"""
    domains = ['zhihu.com', 'weibo.com', 'csdn.net', 'jianshu.com']
    return any(d in urlparse(url).netloc.lower() for d in domains)

def get_content(url):
    """è·å–é¡µé¢å†…å®¹ - æ”¹è¿›ç‰ˆé…ç½®"""
    chrome_options = Options()
    
    # åŸºç¡€é…ç½®
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    
    # ğŸ”‘ User-Agent (å¿…éœ€)
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    # æ ¹æ®ç½‘ç«™ç±»å‹é…ç½®
    stealth = is_social_media(url) or is_complex_site(url)
    
    if stealth:
        # å®Œæ•´åæ£€æµ‹é…ç½® (å¯¹çŸ¥ä¹ç­‰å¤æ‚ç½‘ç«™å¿…éœ€)
        chrome_options.add_argument('--disable-blink-features=AutomationControlled')
        chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    else:
        # æ™®é€šç½‘ç«™ä¼˜åŒ–
        chrome_options.add_argument('--disable-images')
    
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
        
        if stealth:
            # JavaScriptåæ£€æµ‹ (å…³é”®!)
            driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # HTTP Headersè®¾ç½®
            custom_headers = {
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            driver.execute_cdp_cmd('Network.setExtraHTTPHeaders', {'headers': custom_headers})
        
        driver.get(url)
        
        if stealth:
            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
            time.sleep(3)
        else:
            WebDriverWait(driver, 15).until(
                lambda d: d.execute_script("return document.readyState") == "complete"
            )
        
        content = driver.page_source
        title = driver.title
        
        return content, title
        
    except Exception as e:
        print(f"è®¿é—®å¤±è´¥: {e}")
        return None, ""
    finally:
        try:
            driver.quit()
        except:
            pass

def extract_content(html, url, title):
    """æå–å†…å®¹"""
    if not html:
        return {"title": "", "url": url, "content": "è®¿é—®å¤±è´¥"}
    
    soup = BeautifulSoup(html, 'html.parser')
    text = soup.get_text(separator='\n', strip=True)
    
    return {
        "title": title or (soup.find('title').get_text().strip() if soup.find('title') else ""),
        "url": url,
        "content": text,
        "content_type": "text/html"
    }

def main():
    parser = argparse.ArgumentParser(description='Web Agent æ”¹è¿›ç‰ˆ')
    parser.add_argument('url')
    parser.add_argument('-o', '--output')
    parser.add_argument('--pretty', action='store_true')
    args = parser.parse_args()
    
    html, title = get_content(args.url)
    result = extract_content(html, args.url, title)
    
    output = json.dumps(result, ensure_ascii=False, indent=2 if args.pretty else None)
    
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(output)
    else:
        print(output)

if __name__ == "__main__":
    main()
